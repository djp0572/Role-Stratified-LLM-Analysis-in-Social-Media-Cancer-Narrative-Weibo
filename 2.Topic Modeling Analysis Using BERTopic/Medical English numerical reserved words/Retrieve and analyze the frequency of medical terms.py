# -*- coding: utf-8 -*-
"""
extract_english_tokens_v1.py
ä»åŸå§‹ç™Œç—‡æ–‡æœ¬ä¸­æŠ½å–æ‰€æœ‰è‹±æ–‡/è‹±æ–‡+æ•°å­— tokenï¼Œç»Ÿè®¡é¢‘æ¬¡ï¼Œä¸ºåç»­â€œåŒ»å­¦ç™½åå•â€æä¾›ä¾æ®
"""

import pandas as pd
import re
from collections import Counter

# ========== è·¯å¾„é…ç½® ==========
INPUT_PATH = "/kaggle/input/bertopic-predata/content_2nd_label_0.csv"  # åŸå§‹æ–‡ä»¶
OUTPUT_PATH = "/kaggle/working/english_tokens_stats.csv"               # ç»Ÿè®¡ç»“æœè¾“å‡º
TEXT_COL = "content"                                                   # æ–‡æœ¬åˆ—å


# ========== ä¸»ç¨‹åº ==========
def main():
    # 1. è¯»å–æ•°æ®
    df = pd.read_csv(INPUT_PATH)
    assert TEXT_COL in df.columns, f"æœªæ‰¾åˆ°åˆ—ï¼š{TEXT_COL}"
    print(f"ğŸ“‚ åŸå§‹æ•°æ®é‡: {len(df)}")

    # 2. å‡†å¤‡è®¡æ•°å™¨
    #  ALPHA: çº¯è‹±æ–‡ (CT, MRI, EGFR)
    #  ALNUM: è‹±æ–‡+æ•°å­— (CA125, FOLFOX6, PD1)
    alpha_counter = Counter()
    alnum_counter = Counter()

    # å¯è°ƒå‚æ•°ï¼šæœ€çŸ­ token é•¿åº¦ï¼Œé¿å… aã€i è¿™ç§æ— æ„ä¹‰å™ªéŸ³
    MIN_LEN = 2

    # 3. æ­£åˆ™æå– token: è¿ç»­çš„ [A-Za-z0-9]
    pattern = re.compile(r"[A-Za-z0-9]+")

    for i, text in enumerate(df[TEXT_COL].astype(str), start=1):
        tokens = pattern.findall(text)
        for tok in tokens:
            if len(tok) < MIN_LEN:
                continue

            # ç»Ÿä¸€è½¬å¤§å†™åšå½’å¹¶ç»Ÿè®¡ï¼ˆCA125 / ca125 è§†ä¸ºåŒä¸€ tokenï¼‰
            key = tok.upper()

            has_alpha = any(c.isalpha() for c in key)
            has_digit = any(c.isdigit() for c in key)

            if has_alpha and not has_digit:
                # çº¯å­—æ¯ï¼šCT, MRI, ALT, AFP, FOLFOX ç­‰
                alpha_counter[key] += 1
            elif has_alpha and has_digit:
                # å­—æ¯ + æ•°å­—ï¼šCA125, CA199, FOLFOX6, PD1 ç­‰
                alnum_counter[key] += 1
            else:
                # çº¯æ•°å­—è¿™é‡Œæš‚æ—¶ä¸ç®¡ï¼Œä¸»è¦çœ‹â€œè‹±æ–‡ç›¸å…³â€çš„ token
                pass

        if i % 50000 == 0:
            print(f"  å·²å¤„ç† {i} æ¡â€¦")

    # 4. æ±‡æ€»ä¸º DataFrame

    rows = []

    for tok, cnt in alpha_counter.items():
        rows.append({"token": tok, "kind": "ALPHA", "count": cnt})

    for tok, cnt in alnum_counter.items():
        rows.append({"token": tok, "kind": "ALNUM", "count": cnt})

    stats_df = pd.DataFrame(rows)

    # æŒ‰é¢‘æ¬¡ä»é«˜åˆ°ä½æ’åºï¼Œæ–¹ä¾¿ä½ ç­›é€‰
    stats_df = stats_df.sort_values(by="count", ascending=False).reset_index(drop=True)

    # ä¹Ÿå¯ä»¥è®¾ç½®ä¸€ä¸ªæœ€å°é¢‘æ¬¡é˜ˆå€¼ï¼Œæ¯”å¦‚åªä¿ç•™å‡ºç° >= 3 æ¬¡çš„ token
    MIN_COUNT = 2
    stats_df = stats_df[stats_df["count"] >= MIN_COUNT]

    # 5. ä¿å­˜ç»“æœ
    stats_df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")

    print(f"âœ… æŠ½å–å®Œæˆï¼Œå…± {len(stats_df)} ä¸ªè‹±æ–‡/è‹±æ–‡æ•°å­— token")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶ï¼š{OUTPUT_PATH}")
    print("\nğŸ” Top 30 é¢„è§ˆï¼š")
    print(stats_df.head(30))


if __name__ == "__main__":
    main()